<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<table class="imgtable"><tr><td>
<img src="pic_webpage_new.JPG" alt="" width="250px" height="180px" />&nbsp;</td>
<td align="left"><h1>Rodolphe Jenatton</h1>
<p>CTO at Bioptimus. </p>
</td></tr></table>
<div class="infoblock">
<div class="blockcontent">
<p><b>Contact:</b> <tt>rj X  bioptimus Y  com</tt> <i>(with X=@ and Y=.)</i></p>
<p><b><a href="http://scholar.google.com/citations?user=QIR6rygAAAAJ&amp;hl=en">[Google Scholar]</a>
<a href="http://www.informatik.uni-trier.de/~ley/pers/hd/j/Jenatton:Rodolphe.html">[dblp]</a>
<a href="http://fr.linkedin.com/pub/rodolphe-jenatton/4/128/520">[LinkedIn]</a></b> </p>
</div></div>
<div class="infoblock">
<div class="blocktitle">Short bio:</div>
<div class="blockcontent">
<p>In 2011, I finished my Ph.D which I conducted within the <a href="http://www.di.ens.fr/sierra/">Sierra Team</a> of the <a href="http://www.di.ens.fr/">Département d'Informatique of École Normale Supérieure</a>.
I had the chance to be co-supervised by <a href="http://www.di.ens.fr/~fbach/">Francis Bach</a> and <a href="http://certis.enpc.fr/~audibert/">Jean-Yves Audibert</a>. 
I then spent a great year as a postdoctoral researcher with <a href="http://www.di.ens.fr/~aspremon/">Alexandre d'Aspremont</a> at <a href="http://www.cmap.polytechnique.fr/">Ecole Polytechnique</a>. 
From January 2013 until May 2014, I worked for <a href="http://labs.criteo.com/">Criteo</a> where I was in charge of improving the statistical and optimization aspects of the ad prediction engine. 
Until April 2019, I worked as a senior machine learning scientist at Amazon, Berlin, focusing on online learning, Bayesian optimization and auto ML.
After that, I was a senior research scientist in the Berlin Google DeepMind team until December 2023. I am now CTO at <a href="https://www.bioptimus.com/">Bioptimus</a>.</p>
</div></div>
<div class="infoblock">
<div class="blocktitle">Research interests:</div>
<div class="blockcontent">
<p>My research interests revolve around machine learning, statistics, optimization, sparsity, auto-ML and the uncertainty modelling in neural networks. I am more generally excited by how foundation models can help understand the inner workings of biology.</p>
</div></div>
<div class="infoblock">
<div class="blocktitle">Recent reviewing activity:</div>
<div class="blockcontent">
<p>Area chair for ICML 2021, 2022; NeurIPS 2020, 2023; ICLR 2021, 2022, 2023, 2024.</p>
</div></div>
<div class="infoblock">
<div class="blocktitle">Publications:</div>
<div class="blockcontent">
<p><b>Journal:</b></p>
<ul>
<li><p>(2022) L. Carratino, M. Cissé, R. Jenatton, J.-P. Vert. On Mixup Regularization. <i>Journal of Machine Learning Research, 23 (2022) 1-31</i>. <a href="https://www.jmlr.org/papers/volume23/20-1385/20-1385.pdf">[pdf]</a> <a href="https://github.com/google/uncertainty-baselines">[code]</a></p>
</li>
</ul>
<ul>
<li><p>(2022) J. Urquhart Allingham, F. Wenzel, Z. Mariet, B. Mustafa, J. Puigcerver, N. Houlsby, G. Jerfel, V. Fortuin, B. Lakshminarayanan, J. Snoek, D. Tran, C. Riquelme, R. Jenatton. Sparse MoEs meet Efficient Ensembles. <i>Transactions on Machine Learning Research</i>. <a href="https://openreview.net/pdf?id=i0ZM36d2qU">[pdf]</a> <a href="https://github.com/google-research/vmoe">[code]</a></p>
</li>
</ul>
<ul>
<li><p>(2022) V. Fortuin, M. Collier, F. Wenzel, J. Allingham, J. Liu, D. Tran, B. Lakshminarayanan, J. Berent, R. Jenatton, E. Kokiopoulou. Deep Classifiers with Label Noise Modeling and Distance Awareness. <i>Transactions on Machine Learning Research</i>. <a href="https://openreview.net/pdf?id=Id7hTt78FV">[pdf]</a> <a href="https://github.com/google/uncertainty-baselines">[code]</a></p>
</li>
</ul>
<ul>
<li><p>(2015) F. Fogel, R. Jenatton, F. Bach,  A. d'Aspremont. Convex Relaxations for Permutation Problems. <i>SIAM Journal on Matrix Analysis and Application, 36(4):1465-1488, 2015</i>. <a href="http://www.di.ens.fr/~fbach/siam_fogel.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2015) R. Gribonval, R. Jenatton, F. Bach. Sparse and spurious: dictionary learning with noise and outliers. <i>IEEE Transactions on Information Theory, 61(11):6298-6319</i>. <a href="https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7222421&amp;sortType%3Dasc_p_Sequence%26filter%3DAND(p_Publication_Number%3A18)%26pageNumber%3D5%26rowsPerPage%3D50">[ieee]</a><a href="http://arxiv.org/pdf/1407.5155v4.pdf">[pdf on arXiv]</a></p>
</li>
</ul>
<ul>
<li><p>(2015) R. Gribonval, R. Jenatton, F. Bach, M. Kleinsteuber and M. Seibert. Sample complexity of dictionary learning and other matrix factorizations. <i>IEEE Transactions on Information Theory, 61(6):3469-3486</i>. <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=7088631&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7088631">[ieee]</a><a href="http://arxiv.org/pdf/1312.3790v1.pdf">[pdf on arXiv]</a></p>
</li>
</ul>
<ul>
<li><p>(2012) R. Jenatton, A. Gramfort, V. Michel, G. Obozinski, E. Eger, F. Bach and B. Thirion. Multi-scale Mining of fMRI Data with Hierarchical Structured Sparsity. <i>SIAM Journal on Imaging Sciences, 5(3):835-856</i>, 2012. <a href="https://hal.inria.fr/inria-00589785v2/document">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2011) R. Jenatton*, J. Mairal*, G. Obozinski, F. Bach (*Contributed equally). Proximal Methods for Hierarchical Sparse Coding. <i>Journal of Machine Learning Research, 12(Jul):2297-2334</i>. <a href="http://www.jmlr.org/papers/volume12/jenatton11a/jenatton11a.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2011) J. Mairal*, R. Jenatton*, G. Obozinski, F. Bach (*Contributed equally). Convex and Network Flow Optimization for Structured Sparsity. <i>Journal of Machine Learning Research, 12(Sep):2681-2720</i>. <a href="http://www.jmlr.org/papers/volume12/mairal11a/mairal11a.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2011) R. Jenatton, J.-Y. Audibert and F. Bach. Structured Variable Selection with Sparsity-Inducing Norms. <i>Journal of Machine Learning Research, 12(Oct):2777-2824</i>. <a href="http://www.jmlr.org/papers/volume12/jenatton11b/jenatton11b.pdf">[pdf]</a> <a href="./software/StructuredLasso_Matlab_Package_version_1_0_RodolpheJenatton.tar">[code]</a></p>
</li>
</ul>
<p><b>Conference:</b></p>
<ul>
<li><p>(2023) J. Kossen, M. Collier, B. Mustafa, X. Wang, X. Zhai, L. Beyer, A. Steiner, J. Berent, R. Jenatton, E. Kokiopoulou. Three Towers: Flexible Contrastive Learning with Pretrained Image Models. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <a href="https://arxiv.org/pdf/2305.16999.pdf">[pdf]</a> </p>
</li>
</ul>
<ul>
<li><p>(2023) G. Ortiz-Jimenez, M. Collier, A. Nawalgaria, A. D'Amour, J. Berent, R. Jenatton, E. Kokiopoulou. When does Privileged information Explain Away Label Noise? <i>International Conference on Machine Learning (ICML)</i>. <a href="https://openreview.net/pdf?id=CnHxxjqkMi">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2023) M. Dehghani et al <a href="https://proceedings.mlr.press/v202/dehghani23a.html">(full list of authors)</a>. Scaling Vision Transformers to 22 Billion Parameters. <i>International Conference on Machine Learning (ICML)</i>. <a href="https://openreview.net/pdf?id=Lhyy8H75KA">[pdf]</a> <a href="https://blog.research.google/2023/03/scaling-vision-transformers-to-22.html">[blogpost]</a></p>
</li>
</ul>
<ul>
<li><p>(2023) M. Collier*, R. Jenatton*, B. Mustafa, N. Houlsby, J. Berent, E. Kokiopoulou (*Contributed equally). Massively Scaling Heteroscedastic Classifiers. <i>International Conference on Learning Representations (ICLR)</i>. <a href="https://openreview.net/pdf?id=sIoED-yPK9l">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2022) J. Puigcerver, R. Jenatton, C. Riquelme, P. Awasthi, S. Bhojanapalli. On the Adversarial Robustness of Mixture of Experts. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/3effb91593c4fb42b1da1528328eff49-Paper-Conference.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2022) B. Mustafa, C. Riquelme, J. Puigcerver, R. Jenatton, N. Houlsby. Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/3e67e84abf900bb2c7cbd5759bfce62d-Paper-Conference.pdf">[pdf]</a> <a href="https://ai.googleblog.com/2022/06/limoe-learning-multiple-modalities-with.html">[blogpost]</a></p>
</li>
</ul>
<ul>
<li><p>(2022) M. Collier, B. Mustafa, E. Kokiopoulou, R. Jenatton, J. Berent. Transfer and Marginalize: Explaining Away Label Noise with Privileged Information. <i>International Conference on Machine Learning (ICML)</i>. <a href="https://arxiv.org/pdf/2202.09244.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2022) S. Ariafar, J. Gilmer, Z. Nado, J. Snoek, R. Jenatton, G. E. Dahl. Predicting the utility of search spaces for black-box optimization: a simple, budget-aware approach. <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>. <a href="https://proceedings.mlr.press/v151/ariafar22a/ariafar22a.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2021) C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, N. Houlsby. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <a href="https://openreview.net/pdf?id=FrIDgjDOH1u">[pdf]</a> <a href="https://ai.googleblog.com/2022/01/scaling-vision-with-sparse-mixture-of.html">[blogpost]</a> <a href="https://github.com/google-research/vmoe">[code]</a></p>
</li>
</ul>
<ul>
<li><p>(2021) M. Collier, B. Mustafa, E. Kokiopoulou, R. Jenatton, J. Berent. Correlated Input-Dependent Label Noise in Large-Scale Image Classification. <i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>. <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Collier_Correlated_Input-Dependent_Label_Noise_in_Large-Scale_Image_Classification_CVPR_2021_paper.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2021) M. Havasi, R. Jenatton, S. Fort, J.Z. Liu, J. Snoek, B. Lakshminarayanan, A.M. Dai, D. Tran. Training independent subnetworks for robust prediction. <i>International Conference on Learning Representations (ICLR)</i>. <a href="https://openreview.net/pdf?id=OGg9XnKxFAH">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2021) V. Perrone, H. Shen, A. Zolic, I. Shcherbatyi, A. Ahmed, T. Bansal, M. Donini, F. Winkelmolen, R. Jenatton, J.B. Faddoul, B. Pogorzelska, M. Miladinovic, K. Kenthapadi, M. Seeger, C. Archambeau. Amazon SageMaker Automatic Model Tuning: Scalable Black-box Optimization. <i>SIGKDD Conference on Knowledge Discovery and Data Mining</i>  <a href="https://arxiv.org/pdf/2012.08489.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2020) F. Wenzel, J. Snoek, D. Tran, R. Jenatton. Hyperparameter Ensembles for Robustness and Uncertainty Quantification. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <a href="https://papers.nips.cc/paper/2020/file/481fbfa59da2581098e841b7afc122f1-Paper.pdf">[pdf]</a> <a href="https://github.com/google/uncertainty-baselines/tree/master/baselines/cifar">[code]</a></p>
</li>
</ul>
<ul>
<li><p>(2020) F. Wenzel, K. Roth, B. S. Veeling, J. Świątkowski, L. Tran, J. Snoek, S. Mandt, T. Salimans, R. Jenatton, S. Nowozin. How Good is the Bayes Posterior in Deep Neural Networks Really? <i>International Conference on Machine Learning (ICML)</i>. <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3581-Paper.pdf">[pdf]</a> <a href="https://github.com/google-research/google-research/tree/master/cold_posterior_bnn">[code]</a> </p>
</li>
</ul>
<ul>
<li><p>(2020) J. Świątkowski, K. Roth, B. S. Veeling, L. Tran, J. V. Dillon, J. Snoek, S. Mandt, T. Salimans, R. Jenatton, S. Nowozin. The k-tied Normal Distribution: A Compact Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks. <i>International Conference on Machine Learning (ICML)</i>. <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3662-Paper.pdf">[pdf]</a> </p>
</li>
</ul>
<ul>
<li><p>(2019) V. Perrone, H. Shen, M. Seeger, C. Archambeau, R. Jenatton. Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <a href="https://arxiv.org/pdf/1909.12552.pdf">[arxiv]</a></p>
</li>
</ul>
<ul>
<li><p>(2018) V. Perrone, R. Jenatton, M. Seeger, C. Archambeau. Scalable Hyperparameter Transfer learning. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <a href="https://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning.pdf">[pdf]</a> <a href="https://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning-supplemental.zip">[supp]</a></p>
</li>
</ul>
<ul>
<li><p>(2017) R. Jenatton, C. Archambeau, J. Gonzalez, M. Seeger. Bayesian Optimization with Tree-structured Dependencies. <i>International Conference on Machine Learning (ICML)</i>. <a href="http://proceedings.mlr.press/v70/jenatton17a/jenatton17a.pdf">[pdf]</a> <a href="http://proceedings.mlr.press/v70/jenatton17a/jenatton17a-supp.pdf">[supp]</a></p>
</li>
</ul>
<ul>
<li><p>(2016) J. Huang, R. Jenatton, C. Archambeau. Online dual decomposition for performance and delivery-based distributed ad allocation. <i>SIGKDD Conference on Knowledge Discovery and Data Mining</i>. <a href="http://www.kdd.org/kdd2016/papers/files/adf0682-huangA.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2016) R. Jenatton, J. Huang, C. Archambeau. Adaptive Algorithms for Online Convex Optimization with Long-term Constraints. <i>International Conference on Machine Learning (ICML)</i>. <a href="http://proceedings.mlr.press/v48/jenatton16.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2015) A. Freno, M. Saveski, R. Jenatton, C. Archambeau. One-Pass Ranking Models for Low-Latency Product Recommendations. <i>SIGKDD Conference on Knowledge Discovery and Data Mining</i>. <a href="http://www0.cs.ucl.ac.uk/staff/c.archambeau/publ/kdd_af15.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2013) F. Fogel, R. Jenatton, F. Bach, A. d'Aspremont. Convex Relaxations for Permutation Problems. <i>Advances in Neural Information Processing Systems (NIPS)</i>. <a href="http://papers.nips.cc/paper/4986-convex-relaxations-for-permutation-problems.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2012) R. Jenatton*, N. Le Roux*, A. Bordes, G. Obozinski (*Contributed equally). A latent factor model for highly multi-relational data. <i>Advances in Neural Information Processing Systems (NIPS)</i>. <a href="http://papers.nips.cc/paper/4744-a-latent-factor-model-for-highly-multi-relational-data.pdf">[pdf]</a> <a href="https://www.hds.utc.fr/everest/doku.php?id=en:lfmnips12">[code]</a></p>
</li>
</ul>
<ul>
<li><p>(2010) J. Mairal*, R. Jenatton*, G. Obozinski, F. Bach (*Contributed equally). Network Flow Algorithms for Structured Sparsity. <i>Advances in Neural Information Processing Systems (NIPS)</i>. <a href="http://papers.nips.cc/paper/3965-network-flow-algorithms-for-structured-sparsity.pdf">[pdf]</a> <a href="http://spams-devel.gforge.inria.fr/">[code]</a></p>
</li>
</ul>
<ul>
<li><p>(2010) R. Jenatton*, J. Mairal*, G. Obozinski, F. Bach (*Contributed equally). Proximal Methods for Sparse Hierarchical Dictionary Learning. <i>International Conference on Machine Learning (ICML)</i>. <a href="http://www.di.ens.fr/~fbach/icml2010a.pdf">[pdf]</a><a href="http://spams-devel.gforge.inria.fr/">[code]</a></p>
</li>
</ul>
<ul>
<li><p>(2010) R. Jenatton, G. Obozinski, F. Bach. Structured Sparse Principal Component Analysis. <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>. <a href="http://jmlr.org/proceedings/papers/v9/jenatton10a/jenatton10a.pdf">[pdf]</a> <a href="./software/SparseStructuredPCA_MatlabToolbox_V1.0_rjenatton.tar">[code]</a></p>
</li>
</ul>
<p><b>Book chapters:</b></p>
<ul>
<li><p>(2012) F. Bach, R. Jenatton, J. Mairal and G. Obozinski. Structured sparsity through convex optimization. <i>Statistical Science Volume 27, Number 4 (2012), 450-468</i>. <a href="http://projecteuclid.org/euclid.ss/1356098550">[Statistical Science]</a> <a href="http://arxiv.org/pdf/1109.2397v2.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2012) F. Bach, R. Jenatton, J. Mairal and G. Obozinski. Optimization with Sparsity-Inducing Penalties. <i>Foundations and Trends in Machine Learning, 4(1):1-106</i>, 2012. <a href="http://dx.doi.org/10.1561/2200000015">[FOT]</a><a href="http://arxiv.org/pdf/1108.0775.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2011) F. Bach, R. Jenatton, J. Mairal and G. Obozinski. Convex Optimization with Sparsity-Inducing Norms. In <i>S. Sra, S. Nowozin, S. J. Wright., editors, Optimization for Machine Learning, MIT Press</i>, 2011. <a href="http://www.di.ens.fr/~fbach/opt_book.pdf">[pdf]</a></p>
</li>
</ul>
<p><b>Technical reports:</b></p>
<ul>
<li><p>(2023) K. Wang, G. Ortiz-Jimenez, R. Jenatton, M. Collier, E. Kokiopoulou, P. Frossard. Pi-DUAL: Using Privileged Information to Distinguish Clean from Noisy Labels. <i>Technical report, arXiv:2310.06600</i>. <a href="https://arxiv.org/pdf/2310.06600.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2020) P. Das, V. Perrone, N. Ivkin, T. Bansal, Z. Karnin, H. Shen, I. Shcherbatyi, Y. Elor, W. Wu, A. Zolic, T. Lienart, A. Tang, A. Ahmed, J.B. Faddoul, R. Jenatton, F. Winkelmolen, P. Gautier, L. Dirac, A. Perunicic, M. Miladinovic, G. Zappella, C. Archambeau, M. Seeger, B. Dutt, L. Rouesnel. Amazon SageMaker Autopilot: a white box AutoML solution at scale. <i>Technical report, arXiv:2012.08483</i>. <a href="https://arxiv.org/pdf/2012.08483.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2020) M. Collier, B. Mustafa, E. Kokiopoulou, R. Jenatton, J. Berent. A Simple Probabilistic Method for Deep Classification under Input-Dependent Label Noise. <i>Technical report, arXiv:2003.06778</i>. <a href="https://arxiv.org/pdf/2003.06778.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2020) L. Tran, B. S. Veeling, K. Roth, J. Swiatkowski, J. V. Dillon, J. Snoek, S. Mandt, T. Salimans, S. Nowozin, R. Jenatton. Hydra: Preserving Ensemble Diversity for Model Distillation. <i>Technical report, arXiv:2001.04694</i>. <a href="https://arxiv.org/pdf/2001.04694.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2019) V. Perrone, I. Shcherbatyi, R. Jenatton, C. Archambeau, M. Seeger. Constrained Bayesian Optimization with Max-Value Entropy Search. <i>Technical report, arXiv:1910.07003</i>. <a href="https://arxiv.org/pdf/1910.07003.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2016) R. Jenatton, J. Huang, C. Archambeau. Online optimization and regret guarantees for non-additive long-term constraints. <i>Technical report, arXiv:1602.05394</i>. <a href="http://arxiv.org/pdf/1602.05394v1.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2015) R. Jenatton, J. Huang, C. Archambeau. Adaptive Algorithms for Online Convex Optimization with Long-term Constraints. <i>Technical report, arXiv:1512.07422</i>. <a href="http://arxiv.org/pdf/1512.07422v1.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2014) M. Seibert, M. Kleinsteuber, R. Gribonval, R. Jenatton and F. Bach. On The Sample Complexity of Sparse Dictionary Learning. <i>Technical report, arXiv:1403.5112</i>, 2014. <a href="http://arxiv.org/pdf/1403.5112v1.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2012) R. Jenatton, R. Gribonval and F. Bach. Local stability and robustness of sparse dictionary learning in the presence of noise. <i>Technical report, HAL 00737152</i>. <a href="http://hal.inria.fr/docs/00/73/71/52/PDF/localDL_HAL.pdf">[pdf]</a></p>
</li>
</ul>
<p><b>Selected workshops/talks:</b></p>
<ul>
<li><p>(2023) J. Urquhart Allingham, F. Wenzel, Z. Mariet, B. Mustafa, J. Puigcerver, N. Houlsby, G. Jerfel, V. Fortuin, B. Lakshminarayanan, J. Snoek, D. Tran, C. Riquelme, R. Jenatton. Sparse MoEs meet Efficient Ensembles. <i>Amazon StatML Workshop</i>. <a href="./other/sparse_moes_meet_efficient_ensembles_aws_03282023.pdf">[slides]</a></p>
</li>
</ul>
<ul>
<li><p>(2020) F. Wenzel, J. Snoek, D. Tran, R. Jenatton. Hyperparameter Ensembles for Robustness and Uncertainty Quantification. <i>auto-ml seminars.</i> <a href="https://automl-seminars.github.io">[website]</a> <a href="./other/talk_automl_seminar_10082020.pdf">[slides]</a></p>
</li>
</ul>
<ul>
<li><p>(2019) J. Świątkowski, K. Roth, B. S. Veeling, L. Tran, J. V. Dillon, J. Snoek, S. Mandt, T. Salimans, R. Jenatton, S. Nowozin. The k-tied Normal Distribution: A Compact Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks. 2nd Symposium on Advances in Approximate Bayesian Inference, 2019 (best student award) <a href="https://openreview.net/pdf?id=SJezFk2VYH">[pdf]</a>  </p>
</li>
</ul>
<ul>
<li><p>(2018) L. Valkov, R. Jenatton, F. Winkelmolen, C. Archambeau. A simple transfer-learning extension of Hyperband. <i>NeurIPS Workshop on Meta-Learning (MetaLearn 2018)</i>.
<a href="http://metalearning.ml/2018/papers/metalearn2018_paper32_main.pdf">[pdf]</a> <a href="http://metalearning.ml/2018/papers/metalearn2018_paper32_supp.pdf">[supp]</a></p>
</li>
</ul>
<ul>
<li><p>(2017) V. Perrone, R. Jenatton, M. Seeger, C. Archambeau. Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian Optimization with Warm Start. <i>NIPS Workshop on Meta-Learning (MetaLearn 2017)</i>.
<a href="https://arxiv.org/pdf/1712.02902.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2015) Sparse and spurious: dictionary learning with noise and outliers. <i>Optimization and Big Data 2015, Edinburgh</i>. <a href="http://www.maths.ed.ac.uk/~prichtar/Optimization_and_Big_Data_2015/slides/Jenatton.pdf">[slides</a>]</p>
</li>
</ul>
<ul>
<li><p>(2011) R. Jenatton, R. Gribonval, and F. Bach. Local Analysis of Sparse Coding in the Presence of Noise. <i>NIPS Workshop on Sparse Representation and Low-rank Approximation</i>. <a href="http://www.youtube.com/watch?v=lxQBswrFjtA">[video]</a></p>
</li>
</ul>
<ul>
<li><p>(2011) J. Mairal, R. Jenatton, G. Obozinski and F. Bach. Learning Hierarchical and Topographic Dictionaries with Structured Sparsity. In <i>proceeding of the SPIE conference on wavelets and sparsity XIV</i>, 2011. <a href="http://lear.inrialpes.fr/people/mairal/resources/pdf/wavelets.pdf">[pdf]</a></p>
</li>
</ul>
<ul>
<li><p>(2011) R. Jenatton, A. Gramfort, V. Michel, G. Obozinski, F. Bach and B. Thirion. Multi-scale Mining of fMRI Data with Hierarchical Structured Sparsity.<i>International Workshop on Pattern Recognition in Neuroimaging (PRNI)</i>. <a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5961256">[ieee pdf</a>]</p>
</li>
</ul>
<ul>
<li><p>(2010) G. Varoquaux, R. Jenatton, A. Gramfort, G. Obozinski, B. Thirion and F. Bach. Sparse Structured Dictionary Learning for Brain Resting-State Activity Modeling. <i>NIPS Workshop on Practical Applications of Sparse Modeling: Open Issues and New Directions</i>.</p>
</li>
</ul>
<ul>
<li><p>(2009) R. Jenatton, J.-Y. Audibert and F. Bach. Active Set Algorithm for Structured Sparsity-Inducing Norms. <i>OPT 2009: 2nd NIPS Workshop on Optimization for Machine Learning</i>. <a href="./other/OPT2009-jenatton.pdf">[pdf]</a> <a href="./other/OPT2009-jenatton_slides.pdf">[slide]</a> <a href="http://videolectures.net/nipsworkshops09_jenatton_asa/">[video]</a></p>
</li>
</ul>
<p><b>Thesis:</b></p>
<ul>
<li><p>(2011) Structured Sparsity-Inducing Norms: Statistical and Algorithmic Properties with Applications to Neuroimaging. Ph.D thesis. <i>Ecole Normale Supérieure de Cachan. 2011</i>. <a href="http://tel.archives-ouvertes.fr/tel-00673326">[pdf]</a> <a href="./other/phd_talk.pdf">[slides of the defense]</a></p>
</li>
</ul>
<dl>
<dt><i>Awards:</i></dt>
<dd><p>
Winner of the best 2012 Applied Mathematics PhD thesis prize, Fondation Hadamard 2012 <a href="http://www.fondation-hadamard.fr/en/page/668">[more details]</a> </p></dd>
<dt></dt>
<dd><p>Runner-up for the best 2012 Machine Learning PhD thesis, Association Française pour l’Intelligence Artiﬁcielle 2012 <a href="http://www.afia.asso.fr/tiki-index.php?page=Prix+de+Th%C3%A8se+en+Intelligence+Artificielle">[more details]</a></p></dd>
</dl>
</div></div>
<div id="footer">
<div id="footer-text">
Page generated 2023-11-28 11:44:19 CET, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
[sourcelink]
(<a href="2023-11-28 11:44:19 CET">source</a>)
</div>
</div>
</div>
</body>
</html>
